{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting to Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named rep.estimators",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff2a3f54a49d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# this wrapper makes it possible to train on subset of features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSklearnClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhep_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommonutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named rep.estimators"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "#import uproot\n",
    "#import ROOT\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from statsmodels.stats.proportion import proportion_confint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# this wrapper makes it possible to train on subset of features\n",
    "from rep.estimators import SklearnClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from hep_ml.commonutils import train_test_split\n",
    "from hep_ml import uboost, gradientboosting as ugb, losses\n",
    "from toy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data \n",
    "\n",
    "var_names = [\"M\", \"labels\"]\n",
    "\n",
    "nb1, ns = int(1E5), int(1E5) #int(1E4)\n",
    "ns_gausfrac = 0.5 # fraction of signal that's in the Gaussian\n",
    "\n",
    "bkgnd1_m_raw = random_linear(m=1, n=nb1)\n",
    "bkgnd1_m_raw = bkgnd1_m_raw*2-np.ones(bkgnd1_m_raw.shape[0])\n",
    "\n",
    "sign1_m_raw  = np.random.normal(loc=0.5, scale=0.01, size = int(ns*ns_gausfrac)).reshape(-1, 1)\n",
    "sign2_m_raw  = np.random.uniform(low=-1., high=1., size = int(ns*(1-ns_gausfrac))).reshape(-1, 1)\n",
    "\n",
    "\n",
    "bkgnd1_raw = np.column_stack([bkgnd1_m_raw, np.ones(bkgnd1_m_raw.shape[0])])\n",
    "bkgnd1_raw = pd.DataFrame(data=bkgnd1_raw,columns=var_names)\n",
    "\n",
    "sign1_raw = np.column_stack([sign1_m_raw, np.zeros(sign1_m_raw.shape[0])])\n",
    "sign1_raw = pd.DataFrame(data=sign1_raw,columns=var_names)\n",
    "\n",
    "sign2_raw = np.column_stack([sign2_m_raw, np.zeros(sign2_m_raw.shape[0])])\n",
    "sign2_raw = pd.DataFrame(data=sign2_raw,columns=var_names)\n",
    "\n",
    "\n",
    "data = pd.concat([bkgnd1_raw, sign1_raw, sign2_raw])\n",
    "data = data.sample(frac=1) #shuffling\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the Dataset\n",
    "labels = data['labels']\n",
    "data = data.drop('labels', axis=1)\n",
    "trainX, testX, trainY, testY = train_test_split(data, np.array(labels.tolist()), random_state=42)\n",
    "testX_bgnd = testX[testY==1]\n",
    "#Set training weights so the weight given to signal and background events in total is the same\n",
    "wtrainY = np.where(trainY==0, (nb1)*1./ns, 1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the Dataset\n",
    "def plot_1d(varname=\"M\"):\n",
    "    plt.hist(data[labels==1][varname], alpha=0.5)\n",
    "    plt.hist(data[labels==2][varname], alpha=0.5)\n",
    "    plt.hist(data[labels==0][varname], alpha=0.5)\n",
    "    \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 1, 1), plt.title(\"M\"),       plot_1d(varname=\"M\")\n",
    "#plt.subplot(1, 2, 2), plt.title(\"X\"),       plot_1d(varname=\"X\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Training\n",
    "uniform_features  = [\"M\"]\n",
    "train_features = [\"M\"] #Training on the same feature we are trying to make uniform\n",
    "n_estimators = 30 #150\n",
    "base_estimator = DecisionTreeClassifier(max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Classifiers and Training\n",
    "from rep.metaml import ClassifiersFactory\n",
    "\n",
    "classifiers = ClassifiersFactory()\n",
    "\n",
    "base_ada = GradientBoostingClassifier(max_depth=4, n_estimators=n_estimators, learning_rate=0.1)\n",
    "classifiers['AdaBoost'] = SklearnClassifier(base_ada, features=train_features)\n",
    "\n",
    "\n",
    "knnloss = ugb.KnnAdaLossFunction(uniform_features, knn=10, uniform_label=1)\n",
    "ugbKnn = ugb.UGradientBoostingClassifier(loss=knnloss, max_depth=4, n_estimators=n_estimators,\n",
    "                                        learning_rate=0.4, train_features=train_features)\n",
    "classifiers['uGB+knnAda'] = SklearnClassifier(ugbKnn) \n",
    "\n",
    "uboost_clf = uboost.uBoostClassifier(uniform_features=uniform_features, uniform_label=1,\n",
    "                                     base_estimator=base_estimator, \n",
    "                                     n_estimators=n_estimators, train_features=train_features, \n",
    "                                     efficiency_steps=12, n_threads=4)\n",
    "classifiers['uBoost'] = SklearnClassifier(uboost_clf)\n",
    "\n",
    "flatnessloss = ugb.KnnFlatnessLossFunction(uniform_features, fl_coefficient=3., power=1.3, uniform_label=1)\n",
    "ugbFL = ugb.UGradientBoostingClassifier(loss=flatnessloss, max_depth=4, \n",
    "                                       n_estimators=n_estimators, \n",
    "                                       learning_rate=0.1, train_features=train_features)\n",
    "classifiers['uGB+FL'] = SklearnClassifier(ugbFL)\n",
    "\n",
    "\n",
    "classifiers.fit(trainX, trainY, parallel_profile='threads-4', sample_weight=wtrainY) #max_iter=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC as a function of number of estimators\n",
    "from rep.report.metrics import RocAuc\n",
    "report = classifiers.test_on(testX, testY)\n",
    "\n",
    "report.roc().plot(new_plot=True, figsize=[10, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC as Function of Number of Estimators\n",
    "ROC AUC - an area under the ROC curve, the more the better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.ylim(0.95, 1.)\n",
    "#plt.xlim(0., 15.)\n",
    "report.learning_curve(RocAuc(), steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sculpting a background\n",
    "Testing whether a peak is sculpted in background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predictions of various models\n",
    "dpredtestY = {}\n",
    "for clf_name in classifiers:\n",
    "    dpredtestY[clf_name] = classifiers[clf_name].predict(testX_bgnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d_test(clf_name, varname=\"M\", stype=0):\n",
    "    contents_tot, bins, patches = plt.hist(testX_bgnd[varname], alpha=0.5, bins=25)\n",
    "    contents_sig, _   , _       = plt.hist(testX_bgnd[dpredtestY[clf_name]==stype][varname], alpha=0.5, bins=bins)\n",
    "    bin_centers =  (bins[1:]+bins[:-1])/2.\n",
    "    plt.cla()\n",
    "    plt.hist(bin_centers, weights=contents_sig/contents_tot, bins=bins, alpha=0.5)\n",
    "    \n",
    "    plt.title(clf_name)\n",
    "    #plt.hist(testX[testY==0][varname], alpha=0.5)\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(12, 6))\n",
    "for iclf, clf_name in enumerate(classifiers):\n",
    "    plt.subplot(1, len(classifiers), iclf+1), plt.xlabel(\"M\"), plt.ylabel(\"Bgnd Frac Acc\"),  plot_1d_test(clf_name, varname=\"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDE (squared deviation of efficiency) learning curve\n",
    "SDE vs the number of built trees. SDE is a metric of nonuniformity &mdash; less is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hep_ml.metrics import BinBasedSDE, KnnBasedCvM\n",
    "report.learning_curve(BinBasedSDE(uniform_features, uniform_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CvM learning curve\n",
    "CvM is a metric of non-uniformity based on Cramer-von Mises distance. We are using knn (based on neighbours) version here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.learning_curve(KnnBasedCvM(uniform_features, uniform_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
